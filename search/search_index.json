{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Napari Easy Augment Batch DL","text":"<p>\ud83d\ude80 Napari-Easy-Augment-Batch-dl is a plugin for advanced 2D deep learning segmentation.  It support advanced options for augmention, model training and prediction in Napari.  </p>"},{"location":"#features","title":"\ud83d\udd25 Features","text":"<p>\u2705 Batch Processing \u2013 Label, augment, train and predict with multiple images at once. \u2705 Easy Labeling \u2013 Quickly assign labels using the intuitive Napari GUI. \u2705 Advanced Augmentations \u2013 Options for color augmentations, elastic deformation, tunable rescaling and more. \u2705 Plugin multiple DL Frameworks \u2013  A plugin mechanism allows different deep learning frameworks such as Stardist, Cellpose, MobileSam, MicroSam, Monai and others to be integrated.  </p>"},{"location":"#how-it-works","title":"\ud83d\udcd6 How It Works","text":"<p>1\ufe0f\u20e3 Load and Label \u2013 Import images and draw labels. 2\ufe0f\u20e3 Generate Augmentations \u2013 Choose transformations like rotation, flipping, or color augmentation and generate augmentations. 3\ufe0f\u20e3 Train &amp; Predict \u2013 Train different models with same augmentation set and evaluate results.  </p>"},{"location":"#get-started","title":"\ud83d\ude80 Get Started","text":"<ol> <li>Install the plugin:  </li> </ol> <p>You can install the plugin via the pip or the Napari Hub</p> <p>Please note that you will also need to install one or more deep learning frameworks for the plugin to be useful.  See the dependencies page. </p> <pre><code>pip install napari-easy-augment-batch-dl\n</code></pre>"},{"location":"about/","title":"About Napari-Easy-Augment-Batch-dl","text":"<p>Napari-Easy-Augment-Batch-dl is a user-friendly plugin for Napari designed to make labeling, batch image augmentation and model training more reproducible . It provides an intuitive graphical interface integrated with Napari, that allows users to load and label images, apply augmentations, and process large datasets efficiently without requiring programming knowledge.  (While the plugin complements programming by streamlining workflows, I always encourage users to learn programming, as it provides deeper flexibility and control over their data and models).</p>"},{"location":"about/#features","title":"\ud83d\udd0d Features","text":"<ul> <li>Takes advantage of Napari's built in labelling functionality.</li> <li>Supports various augmentation techniques such as rotation, flipping, noise addition, and brightness adjustments.</li> <li>Explicitly saves augmented patches for easier trouble shooting and more repeatable training.  </li> <li>Streamlined workflow with three simple steps: Load &amp; Label, Augmentat, and Train &amp; Predict.</li> <li>Plugin mechanism to support training and prediction with different frameworks (Cellpose, Stardist, SAM, Yolo, UNET, and more)</li> </ul>"},{"location":"about/#development-contributions","title":"\ud83d\udee0\ufe0f Development &amp; Contributions","text":"<p>Napari-Easy-Augment-Batch-dl is an open-source project, and contributions are welcome! If you have feature requests, bug reports, or would like to contribute, visit our GitHub repository.</p>"},{"location":"about/#support","title":"\ud83d\udcde Support","text":"<p>For questions, issues, or feedback, please check our FAQ or open a discussion on image.sc.</p>"},{"location":"augment/","title":"Configure Augmentations","text":"<p>You need to generate augmentations before training. </p>"},{"location":"augment/#augmentation-panel","title":"\ud83c\udf9b\ufe0f Augmentation Panel","text":""},{"location":"augment/#steps","title":"Steps:","text":"<p>1\ufe0f\u20e3 Make sure you draw at least one label box and label objects within it. See here 2\ufe0f\u20e3 Select the augmentation types (rotation, flipping, brightness/contrast, etc.) 3\ufe0f\u20e3 Choose <code>Augment current image</code> or <code>Augment all images</code> 4\ufe0f\u20e3 Verify by looking in the <code>patches</code> sub-directory of your project.</p> <p>After completing above steps you should find a <code>ground truth</code> and <code>input</code> directory has been created in your project in the <code>patches directory</code> </p> <p></p> <p>Within the <code>input</code> directory you should see a collection of patches from your images, in the <code>ground truth</code> directory there should be corresponding patches taken from the label image. </p> <p></p> <p>\ud83d\udd04 Next: Train and Predict </p>"},{"location":"dependencies/","title":"Dependencies","text":"<p>Napari-Easy-Augment-Batch-DL relies on one or more deep learning frameworks being installed in your environment.  On start-up it attempts to import the following deep learning toolkits.  At least one has to be successfully imported</p> <p>Cellpose:  For instance segmentation Stardist:  For instance segmentation Pytorch:  For semantic segmentation Mobile SAM:  An approach that uses Yolo + SAM for instance segmentation. Microsam: </p> <p>Other frameworks can be added via our plugin mechanism (Documentation to come)</p> <p>Below are sets of suggested installation instructions for Linux, Mac M1, and Windows.  Not all dependencies are needed.  You need atleast one deep learning framework, but for example if you would like to work with Cellpose only you do not need Stardist or SAM. </p>"},{"location":"dependencies/#pixi","title":"Pixi","text":"<p>We are working on a pixi installer.  Please see here for a <code>micro_sam/cellpose/monai</code> pixi example, and here for a <code>stardist</code> pixi example.  </p>"},{"location":"dependencies/#condapip-instructions","title":"Conda/PIP instructions","text":"<p>The following conda/pip instructions may work, but could become stale because of transitive dependencies.  Please post on image.sc if you have issues. </p>"},{"location":"dependencies/#linux","title":"Linux","text":"<pre><code>    conda create -n easy_augment_env python=3.11\n    conda activate easy_augment_env\n    conda install pip\n    conda install micro_sam\n    pip install \"napari[all]\" # requires quotation marks to parse the square brackets on Linux, not sure if generalizes\n    pip install albumentations matplotlib scipy tifffile \n    pip install \"tensorflow[and-cuda]\" # as above, requires quotation marks (only needed for stardist)\n    pip install stardist \n    pip install gputools #==0.2.15 may no longer needed if we are happy with np 2\n    pip install edt # pip throws: numba v0.60.0, tensorflow v2.18.0 require lower versions of numpy. Should still work but iffy.\n    pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n    pip install pytorch-lightning\n    pip install git+https://github.com/Project-MONAI/MONAI # as of early spring 2025 need to get the github version of monai if using numpy 2.x \n    pip install cellpose\n    pip install git+https://github.com/True-North-Intelligent-Algorithms/tnia-python.git \n    pip install git+https://github.com/True-North-Intelligent-Algorithms/segment-everything.git\n    pip install git+https://github.com/True-North-Intelligent-Algorithms/napari-easy-augment-batch-dl.git\n    pip install git+https://github.com/True-North-Intelligent-Algorithms/napari-segment-everything.git\n</code></pre>"},{"location":"dependencies/#mac-m1","title":"Mac M1","text":"<pre><code>    conda create -n easy_augment_env python=3.11\n    conda activate easy_augment_env\n    conda install micro_sam\n    pip install \"napari[all]\" # also requires quotes on Mac\n    pip install albumentations matplotlib scipy tifffile \n    pip install \"tensorflow[and-cuda]\" # as above, requires quotation marks. Pip throws a bunch of conflicts, but whatever. (only needed for stardist)\n    pip install stardist\n    pip install gputools # version may no longer be needed if going with numpy 2.x ==0.2.15 # pip dependency incompatibilities\n    pip install edt \n    pip install torch torchvision torchaudio # remove the index flag url\n    pip install pytorch-lightning\n    pip install git+https://github.com/Project-MONAI/MONAI # as of early spring 2025 need to get the github version of monai if using numpy 2.x \n    pip install cellpose\n    pip install git+https://github.com/True-North-Intelligent-Algorithms/segment-everything.git\n    pip install git+https://github.com/True-North-Intelligent-Algorithms/napari-easy-augment-batch-dl.git\n    pip install git+https://github.com/True-North-Intelligent-Algorithms/tnia-python.git \n    pip install git+https://github.com/True-North-Intelligent-Algorithms/napari-segment-everything.git\n</code></pre>"},{"location":"dependencies/#windows","title":"Windows","text":"<p>Note that to use GPU with Stardist in Windows we need two different environments because Windows needs an older version of tensorflow, which is not compatible wither newer version of pytorch based toolkits.   This has led to increasingly convoluted installation instructions for stardist-GPU-windows (as transitive dependencies become an issue).  See this thread.  Please add to it if you run into issues. </p> <p>I've also archived a windows gpu stardist environment.yml here.  You will need both the <code>environment.yml</code> and <code>requirements.txt</code>.  </p>"},{"location":"dependencies/#current-windows-stardist-gpu-environment-instructions","title":"Current Windows stardist GPU environment instructions ()","text":"<pre><code>    conda create -n easy_augment_stardist_env python=3.11\n    conda activate easy_augment_stardist_env\n    pip install numpy==1.26.4 # start with numpy 1.26 and hope nothing upgrades it...\n    pip install \"napari[all]\"\n    pip install albumentations matplotlib scipy tifffile \n    conda install -c conda-forge cudatoolkit=11.8 cudnn=8.1.0\n    pip install \"tensorflow&lt;2.11\"\n    pip install stardist==0.8.5\n    pip install gputools==0.2.15\n    pip install edt\n    pip install reikna==0.8.0 \n    pip install numpy==1.26.4 # in case numpy got upgraded go back (hacky yes, but this is what people are reporting works)\n    pip install numba==0.59.1\n    pip install git+https://github.com/True-North-Intelligent-Algorithms/tnia-python.git \n    pip install git+https://github.com/True-North-Intelligent-Algorithms/napari-easy-augment-batch-dl.git\n</code></pre>"},{"location":"dependencies/#windows-pytorch-environment","title":"Windows pytorch environment","text":"<pre><code>    conda create -n easy_augment_pytorch_env python=3.11\n    conda activate easy_augment_pytorch_env\n    conda install micro_sam\n    pip install \"napari[all]\"\n    pip install albumentations matplotlib scipy tifffile \n    pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n    pip install pytorch-lightning\n    pip install git+https://github.com/Project-MONAI/MONAI # as of early spring 2025 need to get the github version of monai if using numpy 2.x \n    pip install cellpose\n    pip install git+https://github.com/True-North-Intelligent-Algorithms/segment-everything.git\n    pip install git+https://github.com/True-North-Intelligent-Algorithms/napari-easy-augment-batch-dl.git\n    pip install git+https://github.com/True-North-Intelligent-Algorithms/tnia-python.git \n    pip install git+https://github.com/True-North-Intelligent-Algorithms/napari-segment-everything.git\n</code></pre>"},{"location":"faq/","title":"Frequently Asked Questions (FAQ)","text":""},{"location":"faq/#have-a-question-let-us-know-and-we-add-it-to-the-faq","title":"\u2753 Have a question ... let us know and we add it to the FAQ","text":""},{"location":"file_organization/","title":"File Organization","text":""},{"location":"file_organization/#file-organization","title":"File Organization","text":"<p>When you save results from <code>napari-easy-augment-batch-dl</code> the following file structure will be generated.   You can use notebooks to generate these files independent of <code>napari-easy-augment-batch-dl</code>. </p> <p>It is useful to understand this file organization as it will help you find artifacts (such as labels, models, and predictions) generated by <code>napari-easy-augment-batch-dl</code>.</p> <p>annotations:  the annotated images labels: Labels are generated from the 'Label Box' rois.   A annotation will not be made into a label unless a bounding box has been drawn around it. models: Trained models are stored here. patches: Patches are regions extracted from labels and augmented.  You can have potentially thousands of patches. predictions: Predictions generated from models yolo-labels/patches/predictions: These are bounding boxes in Yolo format. </p> <p></p>"},{"location":"load_and_label/","title":"Load and Label","text":""},{"location":"load_and_label/#preparation","title":"Preparation","text":"<p>Prior to using this plugin, put the images you want to work with in a project directory as shown below.  </p> <p></p>"},{"location":"load_and_label/#load-panel","title":"\ud83d\udccc Load Panel","text":"<p>After starting the plugin the first step is to load your images and assign labels.  </p> <p> </p> <p>1\ufe0f\u20e3 Click the Open image directory... button. 2\ufe0f\u20e3 Select the directory that contains your image files.  </p>"},{"location":"load_and_label/#drawing-labels","title":"Drawing Labels","text":"<p>1\ufe0f\u20e3 Select Label box layer and draw a label box that is as large or larger than the desired patch size. 2\ufe0f\u20e3 Select labels layer and Label objects within the label box.</p> <p></p>"},{"location":"load_and_label/#save-results","title":"Save Results","text":"<p>Select <code>Save Results</code> periodically to save the labels you have drawn.  </p> <p> </p> <p>After saving results folders should be generated for different types of deep learning artifacts.  </p> <p></p> <p>Inspect the labels directory to verify labels you have drawn have been saved.  </p>"},{"location":"load_and_label/#_1","title":"Load and Label","text":"<p>\ud83d\udd04 Next: Configure Augmentations</p>"},{"location":"overview/","title":"Overview","text":"<p>Napari-Easy-Augment-Batch-DL consists of a panel that interacts with several different Napari layers.  The below screenshot shows Napari-Easy-Augment-Batch-DL open in Napari.  </p> <p> Main screen showing various layers and annotations.</p>"},{"location":"overview/#panel-groups","title":"Panel Groups","text":"<ol> <li> <p>Load and Label: Load images then use the layers to label.</p> </li> <li> <p>Augment:  Apply augmentations to labels and save patches</p> </li> <li> <p>Train and Predict: Use patches to train models, then use model to generate predictions. </p> </li> </ol>"},{"location":"overview/#napari-easy-augment-batch-layers","title":"Napari-Easy-Augment-Batch Layers","text":"<ol> <li> <p>Images: The image set you are working with.</p> </li> <li> <p>Labels: Manually drawn labels.</p> </li> <li> <p>Predictions: Predictions from the trained model.</p> </li> <li> <p>Label Box: Indicates which regions should be used for training. Manually drawn labels outside of a label box will not be used for training. In the figure above, the blue box is a label box.</p> </li> <li> <p>ml_labels (experimental): Labels used for live ML training. Experiment with live machine learning training using these labels.</p> </li> <li> <p>Object Box (experimental): Annotate individual objects with a bounding box. This type of annotation is used for training YOLO and other object detection methods.</p> </li> <li> <p>Predicted Object Box (experimental): Predicted object box from YOLO or other object detection frameworks.</p> </li> </ol>"},{"location":"predict/","title":"Predict","text":"<p>Predict with builtin or pre-trained models</p>"},{"location":"predict/#choose-a-framework","title":"Choose a framework","text":"<p>Choose frameworks using the dropdown in group 3 (<code>Train/Predict</code>).  Available frameworks will depend on what is installed in the current environment. </p> <p>For example in the framework below we have Cellpose, Microsam, Monai UNET, and Random Forest available on the dropdown.  </p> <p></p>"},{"location":"predict/#choose-builtin-models","title":"Choose builtin models","text":"<p>To choose a builtin model go to the <code>model</code> drop down in the <code>Train/Predict</code> group and choose one of the builtin models.  The below screenshot shows how this works for the <code>Cellpose</code> framework.  The user has the choice to choose between <code>cyto3</code> and <code>tissuenet_cp3</code> (there are many more <code>Cellpose</code> models that could be added at a future date)</p> <p></p>"},{"location":"predict/#load-pretrained-model","title":"Load Pretrained Model","text":"<p>In the <code>Train/Predict</code> group choose <code>Load</code> then browse to the <code>models</code> directory.  Models that were trained with Napari-Easy-Augment-Batch-DL will be stored in the standard format for each framework, in the projects <code>models</code> directory. </p> <ol> <li><code>Cellpose</code> are stored under the <code>models</code> directory in a file without an extension.</li> <li><code>Microsam</code> models are stored in <code>models\\checkpoints\\name_of_model\\best.pt</code></li> <li><code>Pytorch</code> and <code>Monai UNET</code> models are stored under the <code>models</code> directory in a <code>.pt</code> file.</li> <li><code>Stardist</code> models are stored under the <code>models</code> directory in a folder that contains a <code>config.json</code> file and a <code>weights_best.h5</code> file (choose the folder in this case).  </li> </ol> <p>Note.  Models can be loaded from any source as long as they are in the correct folder. </p>"},{"location":"predict/#predict_1","title":"Predict","text":"<p>Hit <code>Predict Current Image</code> or <code>Predict All Images</code> then labels will be generated and added to the <code>Predictions</code> layer. </p> <p></p>"},{"location":"support/","title":"Support","text":"<ol> <li>Post a question on Image.sc</li> <li>Post a question on Napari Zulip </li> <li>Post an issue in the Github repo</li> </ol>"},{"location":"train_and_predict/","title":"Train and Predict","text":"<p>Train a model using your labeled data and make predictions on new images.  </p>"},{"location":"train_and_predict/#training-prediction-panel","title":"\ud83c\udfcb\ufe0f Training &amp; Prediction Panel","text":""},{"location":"train_and_predict/#steps","title":"Steps:","text":"<p>1\ufe0f\u20e3 Choose a model from dropdown and configure parameters. 2\ufe0f\u20e3 Press 'Train network' a popup will appear that allows you to configure additional parameters. 3\ufe0f\u20e3 Hit 'Predict current image' or 'Predict all images' to use the trained model to predict labels.  </p>"},{"location":"train_and_predict/#training-popup","title":"Training popup","text":"<p>After hitting train a popup should appear which allows you to further adjust training parameters.  </p> <p></p> <p>After training (or after loading or setting a model) choose <code>Predict current image</code> or <code>Predict all images</code> to predict.   The <code>prediction</code> layer should be populated with the predictions as shown in the below screen shot.  </p> <p></p> <p>After predicting you need to save the project again and the predictions will be written to disk</p> <p></p> <p>The predictions will be written in your project folder under <code>predictions\\class_0</code>.  </p> <p></p> <p>\ud83d\udd04 Next: Run &amp; Export </p>"}]}